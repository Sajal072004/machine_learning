<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Document</title>
</head>
<body>
  <h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 1: Linear Regression</h2>
<h3>🔹 Introduction</h3>
<p>Linear Regression is a <strong>supervised machine learning</strong> algorithm used for <strong>predicting continuous numerical values</strong> based on the relationship between dependent and independent variables.</p>
<p>It assumes a linear relationship between the variables, meaning the change in output is proportional to the change in input.</p>
<hr>
<h3>🔹 Key Concepts</h3>
<ul>
<li><strong>Dependent Variable (Y):</strong> The target we want to predict.</li>
<li><strong>Independent Variable (X):</strong> The input features.</li>
<li><strong>Line Equation:</strong><br>
[
Y = mX + c
]
Where:
<ul>
<li>( m ) = slope of the line (how steep)</li>
<li>( c ) = y-intercept (where the line crosses the y-axis)</li>
</ul>
</li>
</ul>
<hr>
<h3>🔹 Objective</h3>
<p>Minimize the <strong>error</strong> between actual and predicted values using a <strong>loss function</strong>.</p>
<p>Common loss function:</p>
<ul>
<li><strong>Mean Squared Error (MSE)</strong>:</li>
</ul>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BMSE%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%0A" alt="
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
" /></p>
<p>Where:
<img src="https://i.upmath.me/svg/%0Ay_i%20%3D%20%5Ctext%7Bactual%20value%7D%0A" alt="
y_i = \text{actual value}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Chat%7By%7D_i%20%3D%20%5Ctext%7Bpredicted%20value%7D%0A" alt="
\hat{y}_i = \text{predicted value}
" /></p>
<p><strong>Goal:</strong> Find the best ( m ) and ( c ) that minimizes MSE.</p>
<hr>
<h3>🔹 Example</h3>
<p><strong>Problem:</strong><br>
Predict the price of a house based on its size (in sqft).</p>
<table>
<thead>
<tr>
<th style="text-align:center">Size (sqft)</th>
<th style="text-align:center">Price (in $1000s)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1000</td>
<td style="text-align:center">300</td>
</tr>
<tr>
<td style="text-align:center">1500</td>
<td style="text-align:center">400</td>
</tr>
<tr>
<td style="text-align:center">2000</td>
<td style="text-align:center">500</td>
</tr>
</tbody>
</table>
<p>Linear Regression will learn a line like:<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BPrice%7D%20%3D%200.2%20%5Ctimes%20%5Ctext%7BSize%7D%20%2B%20100%0A" alt="
\text{Price} = 0.2 \times \text{Size} + 100
" /></p>
<p>Thus, for a house of 1800 sqft, predicted price:<br>
<img src="https://i.upmath.me/svg/%0A0.2%20%5Ctimes%201800%20%2B%20100%20%3D%20460%0A" alt="
0.2 \times 1800 + 100 = 460
" /><br>
($460,000)</p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>Price (y-axis)
|
|                        .
|                  .
|             .
|        .
|    .
|________________________ Size (x-axis)
</code></pre>
<p>➔ A straight best-fit line through the data points.</p>
<hr>
<h3>🔹 Advantages</h3>
<ul>
<li>Easy to understand and implement.</li>
<li>Fast and computationally efficient.</li>
<li>Works well when the data has a linear relationship.</li>
</ul>
<hr>
<h3>🔹 Disadvantages</h3>
<ul>
<li>Sensitive to outliers.</li>
<li>Cannot capture complex non-linear relationships.</li>
<li>Assumes constant variance of errors (homoscedasticity).</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ &quot;Best Fit Line&quot; = Linear Regression.<br>
✅ &quot;Predict Numbers&quot; = Linear Regression.<br>
✅ &quot;Minimize Squared Error&quot; = Goal of Linear Regression.</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<p><strong>1. What type of output does Linear Regression predict?</strong><br>
a) Categories<br>
b) Continuous Values ✅<br>
c) Images</p>
<p><strong>2. What is the equation form of Linear Regression?</strong><br>
a) ( Y = mX + c ) ✅<br>
b) ( Y = X^2 )<br>
c) ( Y = e^X )</p>
<p><strong>3. What is the loss function used in Linear Regression?</strong><br>
a) Mean Squared Error ✅<br>
b) Cross Entropy<br>
c) Hinge Loss</p>
<p>Awesome! 🚀<br>
Jumping into <strong>Chapter 2: K-Nearest Neighbors (KNN)</strong> immediately.</p>
<p>Here’s your complete notes:</p>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 2: K-Nearest Neighbors (KNN)</h2>
<h3>🔹 Introduction</h3>
<p>K-Nearest Neighbors (KNN) is a <strong>supervised learning algorithm</strong> used for <strong>classification and regression</strong> problems.<br>
It works by <strong>finding the &quot;k&quot; closest points</strong> to a new input and <strong>making predictions based on their values</strong>.</p>
<p><strong>Main idea:</strong></p>
<blockquote>
<p>&quot;A data point is likely to belong to the same category as its neighbors.&quot;</p>
</blockquote>
<hr>
<h3>🔹 How KNN Works</h3>
<ol>
<li>Choose the number ( k ) (number of neighbors).</li>
<li>Calculate distance (usually <strong>Euclidean Distance</strong>) between new point and all training data.</li>
<li>Pick the <strong>k nearest neighbors</strong>.</li>
<li>
<ul>
<li><strong>For classification:</strong> Assign the most common class among neighbors.</li>
<li><strong>For regression:</strong> Take the average of neighbors’ values.</li>
</ul>
</li>
</ol>
<hr>
<h3>🔹 Important Formulas</h3>
<p><strong>Euclidean Distance Formula:</strong><br>
For two points <img src="https://i.upmath.me/svg/%0A(x_1%2C%20y_1)%0A" alt="
(x_1, y_1)
" />
<img src="https://i.upmath.me/svg/%0A(x_2%2C%20y_2)%0A" alt="
(x_2, y_2)
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0Ad%20%3D%20%5Csqrt%7B(x_2%20-%20x_1)%5E2%20%2B%20(y_2%20-%20y_1)%5E2%7D%0A" alt="
d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}
" /></p>
<p><strong>Manhattan Distance Formula:</strong><br>
<img src="https://i.upmath.me/svg/%0Ad%20%3D%20%7Cx_2%20-%20x_1%7C%20%2B%20%7Cy_2%20-%20y_1%7C%0A" alt="
d = |x_2 - x_1| + |y_2 - y_1|
" /></p>
<hr>
<h3>🔹 Example</h3>
<p>Suppose you have:</p>
<table>
<thead>
<tr>
<th style="text-align:center">Height (cm)</th>
<th style="text-align:center">Weight (kg)</th>
<th style="text-align:center">Class</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">180</td>
<td style="text-align:center">80</td>
<td style="text-align:center">Male</td>
</tr>
<tr>
<td style="text-align:center">160</td>
<td style="text-align:center">55</td>
<td style="text-align:center">Female</td>
</tr>
<tr>
<td style="text-align:center">170</td>
<td style="text-align:center">65</td>
<td style="text-align:center">Male</td>
</tr>
</tbody>
</table>
<p>You want to predict for (165, 60).</p>
<ul>
<li>Find distances.</li>
<li>Pick ( k=3 ) nearest neighbors.</li>
<li>Vote:
<ul>
<li>Male: 2 votes</li>
<li>Female: 1 vote</li>
</ul>
</li>
</ul>
<p>Thus, classified as <strong>Male</strong>.</p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>(X and Y axis showing points)
New point ➔ find nearest dots ➔ majority voting
</code></pre>
<p>➔ Circle around the &quot;k&quot; nearest neighbors.</p>
<hr>
<h3>🔹 Advantages</h3>
<ul>
<li>Simple to understand and easy to implement.</li>
<li>No training phase (lazy learner).</li>
<li>Works well with smaller datasets.</li>
</ul>
<hr>
<h3>🔹 Disadvantages</h3>
<ul>
<li>Slow for large datasets (computes distance to every point).</li>
<li>Sensitive to irrelevant features and the choice of k.</li>
<li>Curse of dimensionality in high dimensions.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ &quot;Neighbors decide my fate!&quot;<br>
✅ &quot;No model training — only distance calculation!&quot;<br>
✅ &quot;Odd k avoids tie situations.&quot;</p>
<hr>
<h3>🔹 Tips for Choosing 'k'</h3>
<ul>
<li>Small ( k ): More sensitive to noise.</li>
<li>Large ( k ): Smoother decision boundaries.</li>
<li>Generally choose odd ( k ) to avoid ties in classification.</li>
</ul>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<p><strong>1. KNN belongs to which type of learning?</strong><br>
a) Supervised ✅<br>
b) Unsupervised<br>
c) Reinforcement</p>
<p><strong>2. What does KNN use to make predictions?</strong><br>
a) Decision trees<br>
b) Distances ✅<br>
c) Gradients</p>
<p><strong>3. What happens if you choose very large ( k )?</strong><br>
a) Overfitting<br>
b) Underfitting ✅<br>
c) No effect</p>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 3: Decision Trees</h2>
<h3>🔹 Introduction</h3>
<p>Decision Trees are a <strong>supervised learning</strong> method for both <strong>classification</strong> and <strong>regression</strong>. They split the data into subsets based on feature values, forming a tree of decisions that leads to predictions at the leaves.</p>
<hr>
<h3>🔹 Key Concepts</h3>
<ul>
<li><strong>Root Node:</strong> Where the tree starts.</li>
<li><strong>Internal Nodes:</strong> Represent feature tests (questions).</li>
<li><strong>Branches:</strong> Outcomes of those tests.</li>
<li><strong>Leaf Nodes:</strong> Final prediction (class label or value).</li>
</ul>
<p>The tree “grows” by choosing the feature split that best separates the data at each node.</p>
<hr>
<h3>🔹 Splitting Criteria &amp; Formulas</h3>
<ol>
<li><strong>Entropy (Information Gain criterion)</strong><br>
Measures impurity in a node:</li>
</ol>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AH(S)%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bc%7D%20p_i%20%5Clog_2%20p_i%0A" alt="
H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
" /></p>
<ul>
<li>$p_i$ = proportion of class $i$ in the set.</li>
</ul>
<p><strong>Information Gain:</strong></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AIG(S%2C%20A)%20%3D%20H(S)%20-%20%5Csum_%7Bv%20%5Cin%20%5Ctext%7BValues%7D(A)%7D%20%5Cfrac%7B%7CS_v%7C%7D%7B%7CS%7C%7D%20H(S_v)%0A" alt="
IG(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|} H(S_v)
" /></p>
<ol start="2">
<li>
<p><strong>Gini Impurity</strong><br>
Alternative to entropy:<br>
<img src="https://i.upmath.me/svg/%0AG(S)%20%3D%201%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bc%7D%20p_i%5E2%0A" alt="
G(S) = 1 - \sum_{i=1}^{c} p_i^2
" /></p>
<p>Lower Gini means purer nodes.</p>
</li>
</ol>
<hr>
<h3>🔹 Example</h3>
<p><strong>Dataset:</strong> Predict whether to <strong>Play Tennis</strong> based on <strong>Outlook</strong>.</p>
<table>
<thead>
<tr>
<th style="text-align:center">Outlook</th>
<th style="text-align:center">Play?</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Sunny</td>
<td style="text-align:center">No</td>
</tr>
<tr>
<td style="text-align:center">Overcast</td>
<td style="text-align:center">Yes</td>
</tr>
<tr>
<td style="text-align:center">Rain</td>
<td style="text-align:center">Yes</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>Step 1:</strong> Compute parent entropy.</li>
<li><strong>Step 2:</strong> Calculate entropy for each Outlook value subset.</li>
<li><strong>Step 3:</strong> Compute Information Gain = Parent Entropy – Weighted Child Entropies.</li>
<li><strong>Step 4:</strong> Choose the split with highest IG (e.g., Outlook).</li>
</ul>
<p>Resulting tree:</p>
<pre><code>[Root: Outlook]
 ├─ Sunny → Leaf: No
 ├─ Overcast → Leaf: Yes
 └─ Rain → Leaf: Yes
</code></pre>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Intuitive and easy to visualize.</li>
<li>Handles both numeric and categorical features.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Prone to overfitting (trees can grow too complex).</li>
<li>Small changes in data can produce very different trees.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “20 Questions Game” – each node asks a question.<br>
✅ “Entropy is chaos” – higher entropy → more mixed classes.</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>Which criterion measures impurity using probabilities and logarithms?</strong><br>
a) Gini Impurity<br>
b) Entropy ✅<br>
c) Euclidean Distance</p>
</li>
<li>
<p><strong>Information Gain is calculated as:</strong><br>
a) Sum of child entropies<br>
b) Parent entropy minus weighted child entropies ✅<br>
c) Product of parent and child entropies</p>
</li>
<li>
<p><strong>A major risk of unpruned decision trees is:</strong><br>
a) Underfitting<br>
b) Overfitting ✅<br>
c) Slow training</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 4: Naïve Bayes</h2>
<h3>🔹 Introduction</h3>
<p>Naïve Bayes is a <strong>probabilistic supervised</strong> learning algorithm based on <strong>Bayes’ Theorem</strong>, which assumes that all features are independent given the class label. It’s widely used for <strong>text classification</strong> (e.g., spam detection).</p>
<hr>
<h3>🔹 Key Concepts</h3>
<ul>
<li><strong>Prior Probability</strong> $P©$: Probability of class $C$ before seeing data.</li>
<li><strong>Likelihood</strong> $P(X|C)$: Probability of observing features $X$ given class $C$.</li>
<li><strong>Evidence</strong> $P(X)$: Overall probability of observing features $X$.</li>
<li><strong>Posterior Probability</strong> $P(C|X)$: Updated probability of class $C$ after observing $X$.</li>
</ul>
<p><strong>Bayes’ Theorem:</strong><br>
<img src="https://i.upmath.me/svg/%0AP(C%7CX)%20%3D%20%5Cfrac%7BP(X%7CC)P(C)%7D%7BP(X)%7D%0A" alt="
P(C|X) = \frac{P(X|C)P(C)}{P(X)}
" /></p>
<p>Since $P(X)$ is constant across classes, we compare:<br>
<img src="https://i.upmath.me/svg/%0A%5Chat%7BC%7D%20%3D%20%5Carg%5Cmax_%7BC%7D%20P(X%7CC)%20P(C)%0A" alt="
\hat{C} = \arg\max_{C} P(X|C) P(C)
" /></p>
<hr>
<h3>🔹 Example: Spam Detection</h3>
<p>Suppose you have these training counts:</p>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Spam Emails (out of 40)</th>
<th>Ham Emails (out of 60)</th>
</tr>
</thead>
<tbody>
<tr>
<td>Contains “free”</td>
<td>30</td>
<td>10</td>
</tr>
<tr>
<td>Contains “win”</td>
<td>20</td>
<td>15</td>
</tr>
</tbody>
</table>
<ul>
<li><strong>P(Spam) = 40/100 = 0.4</strong></li>
<li><strong>P(Ham) = 60/100 = 0.6</strong>
For a new email that contains both “free” and “win”:</li>
</ul>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AP(%5Ctext%7BSpam%7D%7C%5Ctext%7Bfree%2C%20win%7D)%20%5C%3B%20%5Cpropto%20%5C%3B%20P(%5Ctext%7Bfree%7D%7C%5Ctext%7BSpam%7D)%20%5Ctimes%20P(%5Ctext%7Bwin%7D%7C%5Ctext%7BSpam%7D)%20%5Ctimes%20P(%5Ctext%7BSpam%7D)%0A%3D%20%5Cfrac%7B30%7D%7B40%7D%20%5Ctimes%20%5Cfrac%7B20%7D%7B40%7D%20%5Ctimes%200.4%20%3D%200.15%0A" alt="
P(\text{Spam}|\text{free, win}) \; \propto \; P(\text{free}|\text{Spam}) \times P(\text{win}|\text{Spam}) \times P(\text{Spam})
= \frac{30}{40} \times \frac{20}{40} \times 0.4 = 0.15
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0AP(%5Ctext%7BHam%7D%7C%5Ctext%7Bfree%2C%20win%7D)%20%5C%3B%20%5Cpropto%20%5C%3B%20%5Cfrac%7B10%7D%7B60%7D%20%5Ctimes%20%5Cfrac%7B15%7D%7B60%7D%20%5Ctimes%200.6%20%3D%200.025%0A" alt="
P(\text{Ham}|\text{free, win}) \; \propto \; \frac{10}{60} \times \frac{15}{60} \times 0.6 = 0.025
" /></p>
<p>Since $0.15 &gt; 0.025$, classify as <strong>Spam</strong>.</p>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>            +-------+
Features -&gt; | Naïve | -&gt; Class probabilities
            | Bayes |
            +-------+
</code></pre>
<p>➔ Feature independence lets each word vote separately.</p>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Extremely fast to train and predict.</li>
<li>Works well with high-dimensional data (text).</li>
<li>Requires relatively small amount of training data.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>“Naïve” assumption of feature independence is often violated.</li>
<li>Zero probabilities if a word never appears in training (use Laplace smoothing).</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Bayes updates your beliefs.”<br>
✅ “Naïve because features behave independently.”<br>
✅ “Good for Spam vs. Ham!”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>Which assumption does Naïve Bayes make?</strong><br>
a) Features are dependent<br>
b) Features are independent ✅<br>
c) Classes are continuous</p>
</li>
<li>
<p><strong>In Bayes’ Theorem, what is (P©)?</strong><br>
a) Likelihood<br>
b) Prior probability ✅<br>
c) Posterior probability</p>
</li>
<li>
<p><strong>When a feature never appears in training for a class, you should use:</strong><br>
a) No adjustment<br>
b) Laplace smoothing ✅<br>
c) Increase dataset size</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 5: Support Vector Machines (SVM)</h2>
<h3>🔹 Introduction</h3>
<p>Support Vector Machines (SVM) are a powerful supervised learning algorithm primarily used for <strong>classification</strong>, although they can be adapted for <strong>regression</strong>. SVM works by finding the <strong>hyperplane</strong> that best separates the data points of different classes in a high-dimensional space.</p>
<hr>
<h3>🔹 Key Concepts</h3>
<ul>
<li><strong>Hyperplane:</strong> A decision boundary that separates classes.</li>
<li><strong>Support Vectors:</strong> Data points that are closest to the hyperplane, and which determine the position of the hyperplane.</li>
<li><strong>Margin:</strong> The distance between the hyperplane and the support vectors. SVM maximizes this margin to improve classification.</li>
</ul>
<hr>
<h3>🔹 Formulas</h3>
<ol>
<li>
<p><strong>Equation of the Hyperplane:</strong><br>
The hyperplane in $n$-dimensional space is defined by:<br>
<img src="https://i.upmath.me/svg/%0Aw%20%5Ccdot%20x%20%2B%20b%20%3D%200%0A" alt="
w \cdot x + b = 0
" /><br>
Where:</p>
<ul>
<li>$w$ = weight vector</li>
<li>$b$ = bias</li>
</ul>
</li>
<li>
<p><strong>Maximizing the Margin:</strong><br>
The goal is to maximize the margin:<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMargin%7D%20%3D%20%5Cfrac%7B2%7D%7B%5C%7Cw%5C%7C%7D%0A" alt="
\text{Margin} = \frac{2}{\|w\|}
" /><br>
The optimization problem is then:<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMinimize%7D%20%5C%3B%20%5Cfrac%7B1%7D%7B2%7D%20%5C%7Cw%5C%7C%5E2%0A" alt="
\text{Minimize} \; \frac{1}{2} \|w\|^2
" /><br>
Subject to constraints that the data points are correctly classified.</p>
</li>
</ol>
<hr>
<h3>🔹 Example: 2D Classification</h3>
<p>Suppose you have two classes of points, <strong>Red</strong> and <strong>Blue</strong>, in a 2D space. SVM will:</p>
<ol>
<li>Find the line (hyperplane) that best separates the two classes.</li>
<li>Ensure that the distance from the nearest Red and Blue points to the line is maximized.</li>
</ol>
<p>For a set of points, SVM might find a line:</p>
<pre><code>Red |-----------------------| Blue
</code></pre>
<p>Where the line divides the two classes with maximum margin.</p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>Support Vectors (red and blue points) determine the Hyperplane.
           /-------------------&gt; Hyperplane
          /          Margin
  Red --|----------------------------|-- Blue
         \       Support Vectors    /
</code></pre>
<hr>
<h3>🔹 Types of SVM</h3>
<ol>
<li>
<p><strong>Linear SVM:</strong><br>
Used when the data is linearly separable, i.e., a straight line or hyperplane can perfectly separate the classes.</p>
</li>
<li>
<p><strong>Non-linear SVM:</strong><br>
When data is not linearly separable, SVM can be adapted using <strong>kernels</strong> to transform data into a higher-dimensional space where it can be linearly separated.</p>
<p>Common kernels:</p>
<ul>
<li><strong>Polynomial kernel</strong></li>
<li><strong>Radial Basis Function (RBF) kernel</strong></li>
</ul>
</li>
</ol>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Works well for high-dimensional data.</li>
<li>Effective in cases where there is a clear margin of separation.</li>
<li>Robust to overfitting, especially in high-dimensional space.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Computationally expensive, especially with large datasets.</li>
<li>Choice of kernel and regularization parameter (C) significantly affects performance.</li>
<li>Harder to interpret compared to simpler algorithms like decision trees.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Support Vectors are critical to the decision boundary.”<br>
✅ “Maximize the margin to minimize misclassification.”<br>
✅ “Linear vs Non-linear separation — use the right kernel.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>What is the main goal of SVM?</strong><br>
a) Minimize the distance between points<br>
b) Maximize the margin between classes ✅<br>
c) Minimize the number of support vectors</p>
</li>
<li>
<p><strong>What does the kernel trick in SVM allow?</strong><br>
a) To map data to a higher-dimensional space for non-linear separation ✅<br>
b) To separate data linearly without a margin<br>
c) To speed up training</p>
</li>
<li>
<p><strong>What happens if (C) is too large in SVM?</strong><br>
a) Too many support vectors<br>
b) Higher margin but more misclassification<br>
c) Overfitting (tight margin, few misclassifications) ✅</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 6: Overfitting vs Underfitting</h2>
<h3>🔹 Introduction</h3>
<p>Overfitting and underfitting are two key concepts in machine learning that describe how well a model generalizes to new data. Balancing these two is crucial for building a model that performs well on unseen data.</p>
<hr>
<h3>🔹 Overfitting</h3>
<p><strong>Definition:</strong><br>
Overfitting occurs when a model is too complex and captures the <strong>noise</strong> or <strong>random fluctuations</strong> in the training data, instead of generalizing to the underlying trend.</p>
<ul>
<li>
<p><strong>Symptoms of Overfitting:</strong></p>
<ul>
<li>High accuracy on the training data.</li>
<li>Low accuracy on the test data.</li>
</ul>
</li>
<li>
<p><strong>Causes of Overfitting:</strong></p>
<ul>
<li>Too many features relative to the number of training samples.</li>
<li>A very complex model (e.g., a deep decision tree or a very high-degree polynomial).</li>
</ul>
</li>
</ul>
<p><strong>Example:</strong><br>
A decision tree that perfectly classifies all training data but fails on new data is overfitting.</p>
<hr>
<h3>🔹 Underfitting</h3>
<p><strong>Definition:</strong><br>
Underfitting occurs when a model is too simple to capture the underlying trend in the data, leading to poor performance on both the training and test data.</p>
<ul>
<li>
<p><strong>Symptoms of Underfitting:</strong></p>
<ul>
<li>Low accuracy on both training and test data.</li>
</ul>
</li>
<li>
<p><strong>Causes of Underfitting:</strong></p>
<ul>
<li>Too simple a model (e.g., a linear model for a non-linear problem).</li>
<li>Insufficient training (e.g., not enough iterations or too few features).</li>
</ul>
</li>
</ul>
<p><strong>Example:</strong><br>
A linear regression model applied to data with a non-linear relationship between variables will underfit.</p>
<hr>
<h3>🔹 Bias-Variance Tradeoff</h3>
<ul>
<li><strong>Bias:</strong> The error introduced by assuming a simplified model. High bias leads to underfitting.</li>
<li><strong>Variance:</strong> The error introduced by a model’s sensitivity to fluctuations in the training data. High variance leads to overfitting.</li>
</ul>
<p>The goal is to find the right balance between bias and variance:</p>
<ul>
<li><strong>High Bias</strong> → Underfitting</li>
<li><strong>High Variance</strong> → Overfitting</li>
</ul>
<hr>
<h3>🔹 Regularization to Prevent Overfitting</h3>
<p><strong>Regularization</strong> is a technique to prevent overfitting by penalizing large coefficients in the model. The two most common types of regularization are:</p>
<ol>
<li>
<p><strong>L1 Regularization (Lasso):</strong><br>
Adds a penalty proportional to the absolute value of the coefficients. It can lead to sparse models (some coefficients become zero).</p>
<p>Regularized loss function:<br>
<img src="https://i.upmath.me/svg/%0AL_%7B%5Ctext%7BL1%7D%7D%20%3D%20%5Ctext%7BLoss%7D%20%2B%20%5Clambda%20%5Csum%20%7Cw_i%7C%0A" alt="
L_{\text{L1}} = \text{Loss} + \lambda \sum |w_i|
" /></p>
</li>
<li>
<p><strong>L2 Regularization (Ridge):</strong><br>
Adds a penalty proportional to the square of the coefficients, preventing excessively large values.</p>
<p>Regularized loss function:<br>
<img src="https://i.upmath.me/svg/%0AL_%7B%5Ctext%7BL2%7D%7D%20%3D%20%5Ctext%7BLoss%7D%20%2B%20%5Clambda%20%5Csum%20w_i%5E2%0A" alt="
L_{\text{L2}} = \text{Loss} + \lambda \sum w_i^2
" /></p>
</li>
</ol>
<hr>
<h3>🔹 Model Complexity</h3>
<ul>
<li><strong>Simple Models</strong> (e.g., linear regression): Tend to <strong>underfit</strong>.</li>
<li><strong>Complex Models</strong> (e.g., deep neural networks): Tend to <strong>overfit</strong> unless regularized.</li>
</ul>
<p>The goal is to find the <strong>right model complexity</strong> that generalizes well to new data.</p>
<hr>
<h3>🔹 Techniques to Prevent Overfitting</h3>
<ol>
<li>
<p><strong>Cross-Validation</strong>:<br>
Helps assess model performance on different subsets of the data to detect overfitting early.</p>
</li>
<li>
<p><strong>Early Stopping</strong>:<br>
Stop training as soon as the model starts to overfit (in gradient-based models like neural networks).</p>
</li>
<li>
<p><strong>Pruning</strong> (for Decision Trees):<br>
Remove branches of the tree that have little predictive power to reduce complexity.</p>
</li>
<li>
<p>**Dropout (for Neural Networks):<br>
Randomly set a fraction of input units to zero during training to prevent co-adaptation of hidden units.</p>
</li>
</ol>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Understanding overfitting and underfitting helps optimize model performance.</li>
<li>Regularization methods can improve generalization and control model complexity.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Finding the right balance between bias and variance can be challenging.</li>
<li>Complex models might still overfit if not regularized properly.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Overfitting: Too much complexity.”<br>
✅ “Underfitting: Not enough complexity.”<br>
✅ “Balance bias and variance for the best model.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>What happens when a model is overfitting?</strong><br>
a) It performs poorly on both training and test data.<br>
b) It performs well on training data but poorly on test data. ✅<br>
c) It performs well on test data but poorly on training data.</p>
</li>
<li>
<p><strong>What is a common method to prevent overfitting in decision trees?</strong><br>
a) Increase tree depth<br>
b) Pruning ✅<br>
c) Increase the number of features</p>
</li>
<li>
<p><strong>In the bias-variance tradeoff, what happens when the model complexity increases?</strong><br>
a) Bias increases<br>
b) Variance increases ✅<br>
c) Both bias and variance decrease</p>
</li>
</ol>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 7: Neural Networks (Perceptron)</h2>
<h3>🔹 Introduction</h3>
<p>A <strong>perceptron</strong> is the simplest form of a neural network— a single neuron model—that can perform <strong>binary classification</strong> on linearly separable data. It forms the building block of more complex neural networks.</p>
<hr>
<h3>🔹 Key Concepts</h3>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BInputs%7D%20(x_i)%3A%20%5Ctext%7BFeatures%20of%20the%20data%20point.%7D%0A" alt="
\text{Inputs} (x_i): \text{Features of the data point.}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BWeights%7D%20(w_i)%3A%20%5Ctext%7BImportance%20assigned%20to%20each%20feature.%7D%0A" alt="
\text{Weights} (w_i): \text{Importance assigned to each feature.}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BBias%7D%20(b)%3A%20%5Ctext%7BAdjusts%20the%20decision%20boundary.%7D%0A" alt="
\text{Bias} (b): \text{Adjusts the decision boundary.}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BActivation%20Function%7D%3A%20%5Ctext%7BA%20step%20function%20in%20the%20basic%20perceptron%2C%20producing%20an%20output%20of%200%20or%201.%7D%0A" alt="
\text{Activation Function}: \text{A step function in the basic perceptron, producing an output of 0 or 1.}
" /></p>
<hr>
<h3>🔹 Mathematical Model &amp; Formulas</h3>
<ol>
<li>
<p><strong>Weighted Sum (Net Input):</strong><br>
<img src="https://i.upmath.me/svg/%0Az%20%3D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20w_i%20x_i%20%2B%20b%0A" alt="
z = \sum_{i=1}^{n} w_i x_i + b
" /></p>
</li>
<li>
<p><strong>Activation (Step) Function:</strong><br>
<img src="https://i.upmath.me/svg/%0A%5Chat%7By%7D%20%3D%20%0A%5Cbegin%7Bcases%7D%0A%20%201%20%26%20%5Ctext%7Bif%20%7D%20z%20%5Cge%200%20%5C%5C%0A%20%200%20%26%20%5Ctext%7Bif%20%7D%20z%20%3C%200%0A%5Cend%7Bcases%7D%0A" alt="
\hat{y} = 
\begin{cases}
  1 &amp; \text{if } z \ge 0 \\
  0 &amp; \text{if } z &lt; 0
\end{cases}
" /></p>
</li>
<li>
<p><strong>Perceptron Learning Rule (Weight Update):</strong><br>
<img src="https://i.upmath.me/svg/%0Aw_i%20%5Cleftarrow%20w_i%20%2B%20%5Ceta%20(y%20-%20%5Chat%7By%7D)%20x_i%0A" alt="
w_i \leftarrow w_i + \eta (y - \hat{y}) x_i
" /><br>
<img src="https://i.upmath.me/svg/%0Ab%20%5Cleftarrow%20b%20%2B%20%5Ceta%20(y%20-%20%5Chat%7By%7D)%0A" alt="
b \leftarrow b + \eta (y - \hat{y})
" /><br>
Where:</p>
<ul>
<li>$\eta$ = learning rate</li>
<li>$y$ = true label</li>
<li>$\hat{y}$ = predicted label</li>
</ul>
</li>
</ol>
<hr>
<h3>🔹 Example: AND Gate</h3>
<p>Train a perceptron to learn the logical AND function:</p>
<table>
<thead>
<tr>
<th style="text-align:center">$x_1$</th>
<th style="text-align:center">$x_2$</th>
<th style="text-align:center">AND ($y$)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<hr>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BInitialize%20%7D%20w_1%2C%20w_2%2C%20b%20%5Ctext%7B%20to%20small%20random%20values.%7D%0A" alt="
\text{Initialize } w_1, w_2, b \text{ to small random values.}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BUpdate%20weights%20using%20the%20rule%20until%20convergence.%7D%0A" alt="
\text{Update weights using the rule until convergence.}
" /></p>
<p align="center"><img align="center" src="https://i.upmath.me/svg/%0A%5Ctext%7BFinal%20decision%20boundary%20separates%20the%20single%20positive%20%7D%20(1%2C%201)%20%5Ctext%7B%20from%20others.%7D%0A" alt="
\text{Final decision boundary separates the single positive } (1, 1) \text{ from others.}
" /></p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code> Inputs x1 ──┐
            ├─&gt; [ Σ(w·x) + b ] ──&gt; Activation ──&gt; Output ŷ
 Inputs x2 ──┘
</code></pre>
<p>➔ A single neuron computing a weighted sum and threshold.</p>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Simple and fast to train.</li>
<li>Foundation for deeper neural networks.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Can only solve linearly separable problems.</li>
<li>Step activation yields no probability score or gradient, limiting learning techniques.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Perceptron = one neuron classifier.”<br>
✅ “Weight update nudges boundary towards correct side.”<br>
✅ “Only linear separability—AND, OR, but not XOR.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>What activation function does the basic perceptron use?</strong><br>
a) Sigmoid<br>
b) Step Function ✅<br>
c) ReLU</p>
</li>
<li>
<p><strong>Which problem can a single perceptron NOT solve?</strong><br>
a) AND<br>
b) OR<br>
c) XOR ✅</p>
</li>
<li>
<p><strong>In the perceptron learning rule, if the prediction $\hat{y}$ is correct, what happens to the weights?</strong><br>
a) They increase by $\eta x_i$<br>
b) They decrease by $\eta x_i$<br>
c) They remain unchanged ✅</p>
</li>
</ol>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 8: K-Fold Cross Validation</h2>
<h3>🔹 Introduction</h3>
<p>Cross validation is a robust technique to <strong>assess model performance</strong> on unseen data. <strong>K-Fold Cross Validation</strong> splits the dataset into (K) equal parts (“folds”), ensuring each data point is used for both training and validation.</p>
<hr>
<h3>🔹 How It Works</h3>
<ol>
<li><strong>Split</strong> the data into $K$ equal folds.</li>
<li><strong>Iterate</strong> $K$ times:
<ul>
<li>In each iteration, use $K-1$ folds to <strong>train</strong> the model.</li>
<li>Use the <strong>remaining</strong> 1 fold to <strong>validate</strong> the model.</li>
</ul>
</li>
<li><strong>Average</strong> the performance metrics (e.g., accuracy, MSE) across all $K$ runs.</li>
</ol>
<p>This gives a more reliable estimate of how the model generalizes.</p>
<hr>
<h3>🔹 Example</h3>
<p>Suppose you have 100 samples and choose $K=5$.</p>
<ul>
<li>Fold sizes: 20 samples each.</li>
<li>Iteration 1: Train on folds 2–5 (80 samples), test on fold 1 (20 samples).</li>
<li>Iteration 2: Train on folds 1, 3–5, test on fold 2.</li>
<li>…</li>
<li>Iteration 5: Train on folds 1–4, test on fold 5.</li>
</ul>
<p>You might record accuracies: 88%, 90%, 87%, 89%, 91%.<br>
<strong>Average accuracy</strong> = $(88 + 90 + 87 + 89 + 91)/5 = 89%$.</p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>[FOLD 1] [FOLD 2] [FOLD 3] [FOLD 4] [FOLD 5]
   ↘ Train on 2–5 → Validate on 1
   ↘ Train on 1,3–5 → Validate on 2
   ↘ ... 
   ↘ Train on 1–4 → Validate on 5
</code></pre>
<p>➔ Each fold gets to be the test set exactly once.</p>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Reduces bias</strong> from a single train/test split.</li>
<li><strong>Uses all data</strong> for training and validation.</li>
<li>Provides a <strong>stable estimate</strong> of model performance.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li><strong>Computationally expensive</strong>, especially with large (K) or heavy models.</li>
<li><strong>May leak information</strong> if pre-processing (e.g., normalization) isn’t done within each fold.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Rotate the test fold!”<br>
✅ “More folds → less bias, more variance (and compute).”<br>
✅ “Do pre-processing inside each fold to avoid leaks.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>In 10-Fold Cross Validation on 100 samples, how many samples are in each test fold?</strong><br>
a) 5<br>
b) 10 ✅<br>
c) 20</p>
</li>
<li>
<p><strong>What is the main benefit of using K-Fold CV over a single train/test split?</strong><br>
a) Faster training<br>
b) Reduced bias and variance in performance estimate ✅<br>
c) Simpler to implement</p>
</li>
<li>
<p><strong>Which mistake can cause data leakage during K-Fold CV?</strong><br>
a) Shuffling data before splitting<br>
b) Performing normalization on the entire dataset before splitting ✅<br>
c) Averaging metrics across folds</p>
</li>
</ol>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 9: Important ML Formulas</h2>
<h3>🔹 Introduction</h3>
<p>Machine Learning relies heavily on mathematical formulas to <strong>measure error</strong>, <strong>quantify impurity</strong>, and <strong>regularize models</strong>. This chapter collects the <strong>key formulas</strong> you need to master.</p>
<hr>
<h3>🔹 Key Formulas</h3>
<ol>
<li>
<p><strong>Mean Squared Error (MSE)</strong><br>
Measures average squared difference between actual and predicted values.<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMSE%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%0A" alt="
\text{MSE} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
" /></p>
</li>
<li>
<p><strong>Root Mean Squared Error (RMSE)</strong><br>
Square root of MSE, brings error to same units as output.<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BRMSE%7D%20%3D%20%5Csqrt%7B%5Ctext%7BMSE%7D%7D%20%3D%20%5Csqrt%7B%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20(y_i%20-%20%5Chat%7By%7D_i)%5E2%20%7D%0A" alt="
\text{RMSE} = \sqrt{\text{MSE}} = \sqrt{ \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 }
" /></p>
</li>
<li>
<p><strong>Mean Absolute Error (MAE)</strong><br>
Average absolute difference: less sensitive to large errors than MSE.<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMAE%7D%20%3D%20%5Cfrac%7B1%7D%7Bn%7D%20%5Csum_%7Bi%3D1%7D%5E%7Bn%7D%20%7Cy_i%20-%20%5Chat%7By%7D_i%7C%0A" alt="
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
" /></p>
</li>
<li>
<p><strong>Entropy</strong> (Decision Trees)<br>
Impurity measure using probabilities and logs.<br>
<img src="https://i.upmath.me/svg/%0AH(S)%20%3D%20-%5Csum_%7Bi%3D1%7D%5E%7Bc%7D%20p_i%20%5Clog_2%20p_i%0A" alt="
H(S) = -\sum_{i=1}^{c} p_i \log_2 p_i
" /></p>
</li>
<li>
<p><strong>Gini Impurity</strong> (Decision Trees)<br>
Alternative impurity:<br>
<img src="https://i.upmath.me/svg/%0AG(S)%20%3D%201%20-%20%5Csum_%7Bi%3D1%7D%5E%7Bc%7D%20p_i%5E2%0A" alt="
G(S) = 1 - \sum_{i=1}^{c} p_i^2
" /></p>
</li>
<li>
<p><strong>Information Gain</strong><br>
Reduction in entropy after a split on attribute $A$:<br>
<img src="https://i.upmath.me/svg/%0AIG(S%2C%20A)%20%3D%20H(S)%20-%20%5Csum_%7Bv%20%5Cin%20%5Ctext%7BValues%7D(A)%7D%20%5Cfrac%7B%7CS_v%7C%7D%7B%7CS%7C%7D%5C%2CH(S_v)%0A" alt="
IG(S, A) = H(S) - \sum_{v \in \text{Values}(A)} \frac{|S_v|}{|S|}\,H(S_v)
" /></p>
</li>
<li>
<p><strong>Bayes’ Theorem</strong> (Naïve Bayes)<br>
Posterior probability formula:<br>
<img src="https://i.upmath.me/svg/%0AP(C%7CX)%20%3D%20%5Cfrac%7BP(X%7CC)%5C%2CP(C)%7D%7BP(X)%7D%0A" alt="
P(C|X) = \frac{P(X|C)\,P(C)}{P(X)}
" /></p>
</li>
<li>
<p><strong>Linear SVM Margin</strong><br>
Margin width for hyperplane $w \cdot x + b = 0$:<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMargin%7D%20%3D%20%5Cfrac%7B2%7D%7B%5ClVert%20w%20%5CrVert%7D%0A" alt="
\text{Margin} = \frac{2}{\lVert w \rVert}
" /></p>
</li>
<li>
<p><strong>Regularization Penalties</strong></p>
<ul>
<li><strong>L2 (Ridge):</strong><br>
<img src="https://i.upmath.me/svg/%0AL_%7B%5Ctext%7Bridge%7D%7D%20%3D%20%5Ctext%7BLoss%7D%20%2B%20%5Clambda%20%5Csum_%7Bi%7D%20w_i%5E2%0A" alt="
L_{\text{ridge}} = \text{Loss} + \lambda \sum_{i} w_i^2
" /></li>
<li><strong>L1 (Lasso):</strong><br>
<img src="https://i.upmath.me/svg/%0AL_%7B%5Ctext%7Blasso%7D%7D%20%3D%20%5Ctext%7BLoss%7D%20%2B%20%5Clambda%20%5Csum_%7Bi%7D%20%7Cw_i%7C%0A" alt="
L_{\text{lasso}} = \text{Loss} + \lambda \sum_{i} |w_i|
" /></li>
</ul>
</li>
<li>
<p><strong>Principal Component Analysis (PCA)</strong><br>
Compute covariance matrix $ \Sigma $ and eigen-decompose:<br>
<img src="https://i.upmath.me/svg/%0A%5CSigma%20%3D%20%5Cfrac%7B1%7D%7Bn-1%7D%20X%5E%5Ctop%20X%2C%5Cquad%20%5CSigma%20v%20%3D%20%5Clambda%20v%0A" alt="
\Sigma = \frac{1}{n-1} X^\top X,\quad \Sigma v = \lambda v
" /><br>
Principal components are top eigenvectors $v$.</p>
</li>
</ol>
<hr>
<h3>🔹 Example Calculation</h3>
<p><strong>Compute MSE</strong> for actual ${3, 5, 2}$ and predicted ${2, 5, 4}$:<br>
<img src="https://i.upmath.me/svg/%0A%5Ctext%7BMSE%7D%20%3D%20%5Cfrac%7B(3-2)%5E2%20%2B%20(5-5)%5E2%20%2B%20(2-4)%5E2%7D%7B3%7D%0A%3D%20%5Cfrac%7B1%20%2B%200%20%2B%204%7D%7B3%7D%20%3D%20%5Cfrac%7B5%7D%7B3%7D%20%5Capprox%201.67%0A" alt="
\text{MSE} = \frac{(3-2)^2 + (5-5)^2 + (2-4)^2}{3}
= \frac{1 + 0 + 4}{3} = \frac{5}{3} \approx 1.67
" /></p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>                ┌────────────────────────────────┐
                │    Key ML Formulas Cheat-sheet │
                └────────────────────────────────┘
      MSE → squares → average
      MAE → absolutes → average
      Entropy → logs → impurity
      ...
</code></pre>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<ul>
<li>“Square errors → MSE, Root → RMSE.”</li>
<li>“Absolute errors → MAE for robustness.”</li>
<li>“Entropy: chaos measure; Gini: probability-based impurity.”</li>
<li>“Penalty λ shrinks weights in regularization.”</li>
</ul>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>Which metric gives error in the same units as the target?</strong><br>
a) MSE<br>
b) RMSE ✅<br>
c) MAE</p>
</li>
<li>
<p><strong>Entropy is zero when:</strong><br>
a) All samples are in one class ✅<br>
b) Classes are equally mixed<br>
c) No samples</p>
</li>
<li>
<p><strong>L1 regularization adds a penalty of:</strong><br>
a) Sum of squared weights<br>
b) Sum of absolute weights ✅<br>
c) Sum of errors</p>
</li>
<li>
<p><strong>Information Gain measures:</strong><br>
a) Increase in entropy after split<br>
b) Decrease in entropy after split ✅<br>
c) Total entropy</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 10: PCA and LDA</h2>
<h3>🔹 Introduction</h3>
<p><strong>PCA (Principal Component Analysis)</strong> and <strong>LDA (Linear Discriminant Analysis)</strong> are both <strong>dimensionality reduction</strong> techniques, but they serve different goals:</p>
<ul>
<li><strong>PCA</strong> is an <strong>unsupervised</strong> method focused on capturing the directions of maximum <strong>variance</strong> in the data.</li>
<li><strong>LDA</strong> is a <strong>supervised</strong> method focused on finding directions that maximize <strong>class separability</strong>.</li>
</ul>
<hr>
<h3>🔹 Principal Component Analysis (PCA)</h3>
<p><strong>Objective:</strong> Reduce dimensionality by projecting data onto a lower-dimensional space defined by the top principal components (directions of highest variance).</p>
<ol>
<li><strong>Standardize</strong> the data (zero mean, unit variance).</li>
<li>Compute the <strong>covariance matrix</strong>:
<img src="https://i.upmath.me/svg/%0A%5CSigma%20%3D%20%5Cfrac%7B1%7D%7Bn-1%7D%20X%5E%5Ctop%20X%0A" alt="
\Sigma = \frac{1}{n-1} X^\top X
" /></li>
<li><strong>Eigen-decompose</strong> the covariance matrix:
<img src="https://i.upmath.me/svg/%0A%5CSigma%20v%20%3D%20%5Clambda%20v%0A" alt="
\Sigma v = \lambda v
" /></li>
</ol>
<ul>
<li>$v$: eigenvector (principal component)</li>
<li>$\lambda$: eigenvalue (variance explained)</li>
</ul>
<ol start="4">
<li>
<p><strong>Select</strong> top $k$ eigenvectors to form the projection matrix.</p>
</li>
<li>
<p><strong>Project</strong> original data:
<img src="https://i.upmath.me/svg/%0AZ%20%3D%20X%20V_k%0A" alt="
Z = X V_k
" /></p>
</li>
</ol>
<p>Where $V_k$ is the matrix of top $k$ eigenvectors.</p>
<hr>
<h3>🔹 Linear Discriminant Analysis (LDA)</h3>
<p><strong>Objective:</strong> Reduce dimensionality by projecting data onto directions that maximize the <strong>between-class variance</strong> and minimize the <strong>within-class variance</strong>.</p>
<ol>
<li>
<p>Compute <strong>within-class scatter matrix</strong> $S_W$:
<img src="https://i.upmath.me/svg/%0AS_W%20%3D%20%5Csum_%7Bc%3D1%7D%5E%7BC%7D%20%5Csum_%7Bx%20%5Cin%20D_c%7D%20(x%20-%20%5Cmu_c)(x%20-%20%5Cmu_c)%5E%5Ctop%0A" alt="
S_W = \sum_{c=1}^{C} \sum_{x \in D_c} (x - \mu_c)(x - \mu_c)^\top
" /></p>
</li>
<li>
<p>Compute <strong>between-class scatter matrix</strong> $S_B$:
<img src="https://i.upmath.me/svg/%0AS_B%20%3D%20%5Csum_%7Bc%3D1%7D%5E%7BC%7D%20N_c%20(%5Cmu_c%20-%20%5Cmu)(%5Cmu_c%20-%20%5Cmu)%5E%5Ctop%0A" alt="
S_B = \sum_{c=1}^{C} N_c (\mu_c - \mu)(\mu_c - \mu)^\top
" /></p>
</li>
<li>
<p>Solve the <strong>generalized eigenvalue problem</strong>:
<img src="https://i.upmath.me/svg/%0AS_W%5E%7B-1%7D%20S_B%20v%20%3D%20%5Clambda%20v%0A" alt="
S_W^{-1} S_B v = \lambda v
" /></p>
</li>
<li>
<p><strong>Select</strong> eigenvectors corresponding to the top eigenvalues for projection.</p>
</li>
</ol>
<hr>
<h3>🔹 Example</h3>
<p><strong>Dataset:</strong> Two classes (A and B) in 3D space.</p>
<ul>
<li><strong>PCA:</strong> Finds directions where data (both classes) vary the most, regardless of labels.</li>
<li><strong>LDA:</strong> Finds direction that best separates A from B.</li>
</ul>
<p>After PCA:</p>
<pre><code>[3D data] → PC1, PC2 projections (max variance axes)
</code></pre>
<p>After LDA:</p>
<pre><code>[3D data] → LD1 projection (max class separation)
</code></pre>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>PCA:</strong></p>
<ul>
<li><strong>Advantages:</strong>
<ul>
<li>Unsupervised, no class labels needed.</li>
<li>Captures most variance in fewer dimensions.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>May not separate classes well (ignores labels).</li>
</ul>
</li>
</ul>
<p><strong>LDA:</strong></p>
<ul>
<li><strong>Advantages:</strong>
<ul>
<li>Optimizes for class separability.</li>
<li>Useful for classification tasks.</li>
</ul>
</li>
<li><strong>Disadvantages:</strong>
<ul>
<li>Requires class labels.</li>
<li>May overfit if classes have very few samples.</li>
</ul>
</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ <strong>PCA = “Find variation axes.”</strong><br>
✅ <strong>LDA = “Find separation axes.”</strong><br>
✅ <strong>PCA unsupervised, LDA supervised.</strong></p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>PCA chooses components that:</strong><br>
a) Maximize class separation<br>
b) Maximize variance ✅<br>
c) Minimize variance</p>
</li>
<li>
<p><strong>LDA requires:</strong><br>
a) No labels<br>
b) Class labels ✅<br>
c) Features to be uncorrelated</p>
</li>
<li>
<p><strong>Which scatter matrix does LDA maximize?</strong><br>
a) Within-class scatter<br>
b) Between-class scatter ✅<br>
c) Covariance matrix</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 11: Ensemble Techniques</h2>
<h3>🔹 Introduction</h3>
<p>Ensemble methods combine multiple “weak” learners to produce a stronger overall model. By aggregating the predictions of several models, ensembles reduce variance (bagging), bias (boosting), or both (stacking).</p>
<hr>
<h3>🔹 Key Concepts</h3>
<ul>
<li><strong>Weak Learner:</strong> A model slightly better than random guessing (e.g., a shallow decision tree).</li>
<li><strong>Ensemble:</strong> A group of models whose outputs are combined.</li>
<li><strong>Aggregation Methods:</strong>
<ul>
<li><strong>Voting</strong> (classification)</li>
<li><strong>Averaging</strong> (regression)</li>
<li><strong>Stacking</strong> (meta-model learns to combine base models)</li>
</ul>
</li>
</ul>
<hr>
<h3>🔹 Types of Ensemble Methods &amp; Formulas</h3>
<ol>
<li>
<p><strong>Bagging (Bootstrap Aggregating)</strong></p>
<ul>
<li><strong>Process:</strong> Train each model on a random bootstrap sample (with replacement) of the dataset.</li>
<li><strong>Prediction (classification):</strong><br>
<img src="https://i.upmath.me/svg/%0A%5Chat%7By%7D%20%3D%20%5Ctext%7Bmajority%5C_vote%7D%5Cbigl(%5Chat%7By%7D_1%2C%20%5Chat%7By%7D_2%2C%20%5Cdots%2C%20%5Chat%7By%7D_m%5Cbigr)%0A" alt="
\hat{y} = \text{majority\_vote}\bigl(\hat{y}_1, \hat{y}_2, \dots, \hat{y}_m\bigr)
" /></li>
<li><strong>Example:</strong> Random Forest (ensemble of decision trees).</li>
</ul>
</li>
<li>
<p><strong>Boosting</strong></p>
<ul>
<li><strong>Process:</strong> Sequentially train models, each focusing on the errors of the previous.</li>
<li><strong>AdaBoost Update Weight:</strong><br>
<img src="https://i.upmath.me/svg/%0Aw_i%20%5Cleftarrow%20w_i%20%5Ctimes%20%5Cexp%5Cbigl(%5Calpha_t%20%5Ccdot%20%5Cmathbb%7BI%7D(y_i%20%5Cneq%20%5Chat%7By%7D_i)%5Cbigr)%0A" alt="
w_i \leftarrow w_i \times \exp\bigl(\alpha_t \cdot \mathbb{I}(y_i \neq \hat{y}_i)\bigr)
" /><br>
Where $\alpha_t$ is the model’s weight based on its error.</li>
<li><strong>Example:</strong> AdaBoost, Gradient Boosting, XGBoost.</li>
</ul>
</li>
<li>
<p><strong>Stacking</strong></p>
<ul>
<li><strong>Process:</strong> Train several base learners on the full dataset; then train a “meta-learner” on their predictions to learn optimal combinations.</li>
<li><strong>Meta-Prediction:</strong><br>
<img src="https://i.upmath.me/svg/%0A%5Chat%7By%7D%20%3D%20g%5Cbigl(%5Chat%7By%7D_1%2C%20%5Chat%7By%7D_2%2C%20%5Cdots%2C%20%5Chat%7By%7D_m%5Cbigr)%0A" alt="
\hat{y} = g\bigl(\hat{y}_1, \hat{y}_2, \dots, \hat{y}_m\bigr)
" /><br>
Where $g$ is the meta-model.</li>
</ul>
</li>
</ol>
<hr>
<h3>🔹 Example: Random Forest Classification</h3>
<ul>
<li><strong>Data:</strong> Predict if a customer will churn based on features like tenure, usage, support calls.</li>
<li><strong>Steps:</strong>
<ol>
<li>Build 100 decision trees on different bootstrapped samples.</li>
<li>Each tree votes “Churn” or “No Churn.”</li>
<li>Final prediction = majority vote.</li>
</ol>
</li>
</ul>
<p>Random Forest reduces overfitting of single trees and handles nonlinearities.</p>
<hr>
<h3>🔹 Important Diagram (Imagine)</h3>
<pre><code>   Data ─┬─&gt; Tree 1 ─┐
         ├─&gt; Tree 2 ─┤
         ├─&gt; ...     ├─&gt; Majority Vote → Final Prediction
         └─&gt; Tree m ─┘
</code></pre>
<p>➔ Many trees vote together.</p>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li><strong>Bagging:</strong> Lowers variance, robust to overfitting.</li>
<li><strong>Boosting:</strong> Lowers bias, often achieves high accuracy.</li>
<li><strong>Stacking:</strong> Can capture complex relationships between models.</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>Increased <strong>computational cost</strong> (many models).</li>
<li>Harder to <strong>interpret</strong> compared to single models.</li>
<li>Overfitting can still occur in boosting if not regularized.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “Bagging = Bootstrap + Aggregation → Lower variance.”<br>
✅ “Boosting = Sequential focus on errors → Lower bias.”<br>
✅ “Stacking = Meta-learner → Learn to combine.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>Which ensemble method reduces variance by training on bootstrapped samples?</strong><br>
a) Boosting<br>
b) Bagging ✅<br>
c) Stacking</p>
</li>
<li>
<p><strong>In boosting, each new learner is trained to:</strong><br>
a) Ignore previous errors<br>
b) Correct the errors of the previous model ✅<br>
c) Use a different dataset</p>
</li>
<li>
<p><strong>Stacking uses a meta-learner to:</strong><br>
a) Generate bootstrapped samples<br>
b) Combine base learner predictions ✅<br>
c) Prune decision trees</p>
</li>
</ol>
<hr>
<hr>
<h1>📚 Machine Learning Exam Notes</h1>
<h2>Chapter 12: New Advancements in ML</h2>
<h3>🔹 Introduction</h3>
<p>Machine Learning is a rapidly evolving field. Here are some of the <strong>latest advancements</strong> you should know:</p>
<ol>
<li><strong>Deep Learning Architectures</strong></li>
<li><strong>Transfer Learning</strong></li>
<li><strong>AutoML</strong></li>
<li><strong>Reinforcement Learning Breakthroughs</strong></li>
<li><strong>Explainable AI (XAI)</strong></li>
<li><strong>Federated Learning</strong></li>
<li><strong>Graph Neural Networks (GNNs)</strong></li>
</ol>
<hr>
<h3>🔹 1. Deep Learning Architectures</h3>
<ul>
<li><strong>Convolutional Neural Networks (CNNs):</strong> Excellent for image data; use convolution and pooling layers.</li>
<li><strong>Recurrent Neural Networks (RNNs) &amp; LSTMs:</strong> Handle sequential data like text and time series.</li>
<li><strong>Transformers:</strong> State-of-the-art for language tasks (e.g., BERT, GPT); rely on self-attention mechanism.</li>
</ul>
<p><strong>Key Formula (Backpropagation Update):</strong><br>
<img src="https://i.upmath.me/svg/%0Aw%20%5Cleftarrow%20w%20-%20%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20w%7D%0A" alt="
w \leftarrow w - \eta \frac{\partial L}{\partial w}
" /><br>
Where $L$ is the loss (e.g., cross-entropy) and $\eta$ is the learning rate.</p>
<hr>
<h3>🔹 2. Transfer Learning</h3>
<ul>
<li><strong>Concept:</strong> Use a pre-trained model on a large dataset, then fine-tune on your target task.</li>
<li><strong>Examples:</strong>
<ul>
<li><strong>ImageNet</strong> pre-trained CNNs (ResNet, VGG) for new vision tasks.</li>
<li><strong>BERT</strong>, <strong>GPT</strong> for NLP tasks.</li>
</ul>
</li>
</ul>
<hr>
<h3>🔹 3. AutoML</h3>
<ul>
<li><strong>Objective:</strong> Automate the end-to-end process of model selection, feature engineering, and hyperparameter tuning.</li>
<li><strong>Tools:</strong> Google’s AutoML, AutoKeras, TPOT.</li>
<li><strong>Benefit:</strong> Democratizes ML, reduces manual effort.</li>
</ul>
<hr>
<h3>🔹 4. Reinforcement Learning (RL) Breakthroughs</h3>
<ul>
<li><strong>Deep Q-Networks (DQN):</strong> Combined Q-learning with deep neural networks for Atari games.</li>
<li><strong>AlphaZero:</strong> Self-play algorithm mastering Chess, Go, and Shogi without human data.</li>
<li><strong>Policy Gradients &amp; Actor-Critic Methods:</strong> Continuous action spaces (e.g., PPO, A3C).</li>
</ul>
<hr>
<h3>🔹 5. Explainable AI (XAI)</h3>
<ul>
<li><strong>Purpose:</strong> Make “black-box” models interpretable.</li>
<li><strong>Techniques:</strong>
<ul>
<li><strong>LIME:</strong> Local surrogate models to explain individual predictions.</li>
<li><strong>SHAP:</strong> Shapley values from cooperative game theory to assign feature importance.</li>
</ul>
</li>
</ul>
<hr>
<h3>🔹 6. Federated Learning</h3>
<ul>
<li><strong>Concept:</strong> Train models across decentralized devices without sharing raw data (privacy-preserving).</li>
<li><strong>Use Case:</strong> Mobile keyboard prediction, health data analytics.</li>
</ul>
<hr>
<h3>🔹 7. Graph Neural Networks (GNNs)</h3>
<ul>
<li><strong>Objective:</strong> Learn from graph-structured data (social networks, molecules).</li>
<li><strong>Basic Update Rule:</strong><br>
<img src="https://i.upmath.me/svg/%0Ah_v%5E%7B(k%2B1)%7D%20%3D%20%5Csigma%20%5CBigl(%20%5Csum_%7Bu%20%5Cin%20%5Cmathcal%7BN%7D(v)%7D%20W%5E%7B(k)%7D%20h_u%5E%7B(k)%7D%20%2B%20b%5E%7B(k)%7D%5CBigr)%0A" alt="
h_v^{(k+1)} = \sigma \Bigl( \sum_{u \in \mathcal{N}(v)} W^{(k)} h_u^{(k)} + b^{(k)}\Bigr)
" /><br>
Where $h_v$ is node embedding and $\mathcal{N}(v)$ are neighbors.</li>
</ul>
<hr>
<h3>🔹 Advantages &amp; Disadvantages</h3>
<p><strong>Advantages:</strong></p>
<ul>
<li>Cutting-edge performance across vision, language, and sequential tasks.</li>
<li>Transfer learning and AutoML accelerate development.</li>
<li>XAI and federated learning address real-world concerns (interpretability, privacy).</li>
</ul>
<p><strong>Disadvantages:</strong></p>
<ul>
<li>High computational cost and resource requirements.</li>
<li>Complexity increases risk of overfitting and deployment challenges.</li>
<li>XAI methods may oversimplify explanations.</li>
</ul>
<hr>
<h3>🔹 Short Tricks to Remember</h3>
<p>✅ “CNNs for images, RNNs for sequences, Transformers for everything!”<br>
✅ “Transfer = reuse knowledge.”<br>
✅ “AutoML = ML on autopilot.”<br>
✅ “XAI = trust through transparency.”<br>
✅ “Federated = train without data leaving.”<br>
✅ “GNNs = ML on graphs.”</p>
<hr>
<h3>🔹 Quick MCQs (Practice)</h3>
<ol>
<li>
<p><strong>Which architecture uses self-attention for language tasks?</strong><br>
a) RNN<br>
b) Transformer ✅<br>
c) CNN</p>
</li>
<li>
<p><strong>Federated Learning primarily addresses which concern?</strong><br>
a) Model accuracy<br>
b) Data privacy ✅<br>
c) Overfitting</p>
</li>
<li>
<p><strong>SHAP is used for:</strong><br>
a) Dimensionality reduction<br>
b) Model interpretability ✅<br>
c) Hyperparameter tuning</p>
</li>
<li>
<p><strong>Graph Neural Networks operate on:</strong><br>
a) Grid data<br>
b) Sequential data<br>
c) Graph-structured data ✅</p>
</li>
</ol>
<hr>
</body>
</html>
